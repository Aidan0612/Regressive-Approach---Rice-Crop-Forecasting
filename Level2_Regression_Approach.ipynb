{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzxJUK_hmrRK",
        "outputId": "da9b99ec-74f7-45a0-94a6-88955075d5f1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    121\u001b[0m       'TBE_EPHEM_CREDS_ADDR'] if ephemeral else _os.environ['TBE_CREDS_ADDR']\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    124\u001b[0m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "CzxJUK_hmrRK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aa63a1b-c5f6-4b5b-89b7-d017f974fb29"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pip install ipyleaflet pystac rich tqdm odc-geo pystac-client planetary-computer odc-stac catboost optuna pyarrow optuna-dashboard darts entropy scipy pyentrp"
      ],
      "id": "9aa63a1b-c5f6-4b5b-89b7-d017f974fb29"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc399a2a-afc0-41a4-8558-c5ae1d463a07"
      },
      "source": [
        "## Import Markdown"
      ],
      "id": "cc399a2a-afc0-41a4-8558-c5ae1d463a07"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "422fa2f5-fa89-4900-8b2a-90a815d99871"
      },
      "outputs": [],
      "source": [
        "# Supress Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "# import ipyleaflet\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Data Science\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Feature Engineering\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score,classification_report,confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Planetary Computer Tools\n",
        "import pystac\n",
        "import pystac_client\n",
        "import odc\n",
        "from pystac_client import Client\n",
        "from pystac.extensions.eo import EOExtension as eo\n",
        "from odc.stac import stac_load\n",
        "import planetary_computer as pc\n",
        "pc.settings.set_subscription_key('c861000c00fb430494b6ced2d9b15cf3')\n",
        "\n",
        "# Others\n",
        "import requests\n",
        "import rich.table\n",
        "from itertools import cycle\n",
        "from tqdm import tqdm\n",
        "from catboost import CatBoostClassifier\n",
        "tqdm.pandas()\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# # Import everything from darts\n",
        "# from darts.dataprocessing.transformers import Scaler, MissingValuesFiller\n",
        "# # from darts.dataprocessing.pipeline import Pipeline\n",
        "# from darts import TimeSeries\n",
        "# from darts.utils.timeseries_generation import gaussian_timeseries, linear_timeseries\n",
        "# #from darts.models import RNNModel, TCNModel, TransformerModel, NBEATSModel, BlockRNNModel\n",
        "# from darts.metrics import mape, smape"
      ],
      "id": "422fa2f5-fa89-4900-8b2a-90a815d99871"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ad17ba-636f-4ff4-b906-4805c05dbdf4"
      },
      "source": [
        "### Utilities function"
      ],
      "id": "29ad17ba-636f-4ff4-b906-4805c05dbdf4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26a9c219-b865-44d3-8930-a15027814159"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "from pyentrp import entropy as ent\n",
        "# Load data into a pandas DataFrame\n",
        "\n",
        "# Calculate statistics for each column\n",
        "def compute_stats(df) :\n",
        "    stats = {}\n",
        "    for col in df.columns:\n",
        "        stats[f'min_{col}'] = df[col].min()\n",
        "        stats[f'max_{col}'] = df[col].max()\n",
        "        stats[f'range_{col}'] = df[col].max() - df[col].min()\n",
        "        stats[f'mean_{col}'] = df[col].mean()\n",
        "        if col != 'index':\n",
        "            corr, _ = pearsonr(df[col], df['index'])\n",
        "            stats[f'correlation_{col}'] = corr\n",
        "            std_ts = df[col].std()\n",
        "            stats[f'permutation_entropy_{col}'] = ent.permutation_entropy(df[col].values, order=4, delay=0.2*std_ts)\n",
        "\n",
        "    return pd.DataFrame.from_dict(results, orient='index')"
      ],
      "id": "26a9c219-b865-44d3-8930-a15027814159"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36df2217-f3ce-45fb-831f-e19cd465d700"
      },
      "outputs": [],
      "source": [
        "def ordinal_distribution(data, dx=3, dy=1, taux=1, tauy=1, return_missing=False, tie_precision=None):\n",
        "    '''\n",
        "    Returns\n",
        "    -------\n",
        "     : tuple\n",
        "       Tuple containing two arrays, one with the ordinal patterns occurring in data \n",
        "       and another with their corresponding probabilities.\n",
        "       \n",
        "    Attributes\n",
        "    ---------\n",
        "    data : array \n",
        "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
        "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
        "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`.\n",
        "    dx : int\n",
        "         Embedding dimension (horizontal axis) (default: 3).\n",
        "    dy : int\n",
        "         Embedding dimension (vertical axis); it must be 1 for time series \n",
        "         (default: 1).\n",
        "    taux : int\n",
        "           Embedding delay (horizontal axis) (default: 1).\n",
        "    tauy : int\n",
        "           Embedding delay (vertical axis) (default: 1).\n",
        "    return_missing: boolean\n",
        "                    If `True`, it returns ordinal patterns not appearing in the \n",
        "                    symbolic sequence obtained from **data** are shown. If `False`,\n",
        "                    these missing patterns (permutations) are omitted \n",
        "                    (default: `False`).\n",
        "    tie_precision : int\n",
        "                    If not `None`, **data** is rounded with `tie_precision`\n",
        "                    number of decimals (default: `None`).\n",
        "   \n",
        "    '''\n",
        "    def setdiff(a, b):\n",
        "        '''\n",
        "        Returns\n",
        "        -------\n",
        "        : array\n",
        "            An array containing the elements in `a` that are not contained in `b`.\n",
        "            \n",
        "        Parameters\n",
        "        ----------    \n",
        "        a : tuples, lists or arrays\n",
        "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
        "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
        "        b : tuples, lists or arrays\n",
        "            Array in the format :math:`[[x_{21}, x_{22}, x_{23}, \\\\ldots, x_{2m}], \n",
        "            \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, ..., x_{nm}]]`.\n",
        "        '''\n",
        "\n",
        "        a = np.asarray(a).astype('int64')\n",
        "        b = np.asarray(b).astype('int64')\n",
        "\n",
        "        _, ncols = a.shape\n",
        "\n",
        "        dtype={'names':['f{}'.format(i) for i in range(ncols)],\n",
        "            'formats':ncols * [a.dtype]}\n",
        "\n",
        "        C = np.setdiff1d(a.view(dtype), b.view(dtype))\n",
        "        C = C.view(a.dtype).reshape(-1, ncols)\n",
        "\n",
        "        return(C)\n",
        "\n",
        "    try:\n",
        "        ny, nx = np.shape(data)\n",
        "        data   = np.array(data)\n",
        "    except:\n",
        "        nx     = np.shape(data)[0]\n",
        "        ny     = 1\n",
        "        data   = np.array([data])\n",
        "\n",
        "    if tie_precision is not None:\n",
        "        data = np.round(data, tie_precision)\n",
        "\n",
        "    partitions = np.concatenate(\n",
        "        [\n",
        "            [np.concatenate(data[j:j+dy*tauy:tauy,i:i+dx*taux:taux]) for i in range(nx-(dx-1)*taux)] \n",
        "            for j in range(ny-(dy-1)*tauy)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    symbols = np.apply_along_axis(np.argsort, 1, partitions)\n",
        "    symbols, symbols_count = np.unique(symbols, return_counts=True, axis=0)\n",
        "\n",
        "    probabilities = symbols_count/len(partitions)\n",
        "\n",
        "    if return_missing==False:\n",
        "        return symbols, probabilities\n",
        "    \n",
        "    else:\n",
        "        all_symbols   = list(map(list,list(itertools.permutations(np.arange(dx*dy)))))\n",
        "        miss_symbols  = setdiff(all_symbols, symbols)\n",
        "        symbols       = np.concatenate((symbols, miss_symbols))\n",
        "        probabilities = np.concatenate((probabilities, np.zeros(miss_symbols.__len__())))\n",
        "        \n",
        "        return symbols, probabilities"
      ],
      "id": "36df2217-f3ce-45fb-831f-e19cd465d700"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66156f30-9e91-4aaa-9717-f22668bf1004"
      },
      "outputs": [],
      "source": [
        "def permutation_entropy(data, dx=3, dy=1, taux=1, tauy=1, base=2, normalized=True, probs=False, tie_precision=None):\n",
        "    '''\n",
        "    Returns Permutation Entropy\n",
        "    Attributes:\n",
        "    data : array\n",
        "           Array object in the format :math:`[x_{1}, x_{2}, x_{3}, \\\\ldots ,x_{n}]`\n",
        "           or  :math:`[[x_{11}, x_{12}, x_{13}, \\\\ldots, x_{1m}],\n",
        "           \\\\ldots, [x_{n1}, x_{n2}, x_{n3}, \\\\ldots, x_{nm}]]`\n",
        "           or an ordinal probability distribution (such as the ones returned by :func:`ordpy.ordinal_distribution`).\n",
        "    dx :   int\n",
        "           Embedding dimension (horizontal axis) (default: 3).\n",
        "    dy :   int\n",
        "           Embedding dimension (vertical axis); it must be 1 for time series (default: 1).\n",
        "    taux : int\n",
        "           Embedding delay (horizontal axis) (default: 1).\n",
        "    tauy : int\n",
        "           Embedding delay (vertical axis) (default: 1).\n",
        "    base : str, int\n",
        "           Logarithm base in Shannon's entropy. Either 'e' or 2 (default: 2).\n",
        "    normalized: boolean\n",
        "                If `True`, permutation entropy is normalized by its maximum value \n",
        "                (default: `True`). If `False`, it is not.\n",
        "    probs : boolean\n",
        "            If `True`, assumes **data** is an ordinal probability distribution. If \n",
        "            `False`, **data** is expected to be a one- or two-dimensional \n",
        "            array (default: `False`). \n",
        "    tie_precision : int\n",
        "                    If not `None`, **data** is rounded with `tie_precision`\n",
        "                    number of decimals (default: `None`).\n",
        "    '''\n",
        "    if not probs:\n",
        "        _, probabilities = ordinal_distribution(data, dx, dy, taux, tauy, return_missing=False, tie_precision=tie_precision)\n",
        "    else:\n",
        "        probabilities = np.asarray(data)\n",
        "        probabilities = probabilities[probabilities>0]\n",
        "\n",
        "    if normalized==True and base in [2, '2']:        \n",
        "        smax = np.log2(float(np.math.factorial(dx*dy)))\n",
        "        s    = -np.sum(probabilities*np.log2(probabilities))\n",
        "        return s/smax\n",
        "         \n",
        "    elif normalized==True and base=='e':        \n",
        "        smax = np.log(float(np.math.factorial(dx*dy)))\n",
        "        s    = -np.sum(probabilities*np.log(probabilities))\n",
        "        return s/smax\n",
        "    \n",
        "    elif normalized==False and base in [2, '2']:\n",
        "        return -np.sum(probabilities*np.log2(probabilities))\n",
        "    else:\n",
        "        return -np.sum(probabilities*np.log(probabilities))"
      ],
      "id": "66156f30-9e91-4aaa-9717-f22668bf1004"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6989e88-16d2-4220-aaa6-1cc1f63dd188"
      },
      "outputs": [],
      "source": [
        "def generate_stastical_features(dataframe):\n",
        "    '''\n",
        "    Returns a  list of statistical features such as min,max,range,mean,auto-correlation,permutation entropy for each of the features\n",
        "    Attributes:\n",
        "    dataframe - DataFrame consisting of VV,VH and VV/VH for a time period\n",
        "    '''\n",
        "    features_list = []\n",
        "    for index, row in dataframe.iterrows():\n",
        "        for col in row: \n",
        "            min_vv = min(col)\n",
        "            max_vv = max(col)\n",
        "            range_vv = max_vv - min_vv\n",
        "            mean_vv = np.mean(col)\n",
        "            # print(correlation_vv)\n",
        "            # permutation_entropy_vv = permutation_entropy(col, dx=6,base=2, normalized=True) \n",
        "            features_list.append([min_vv, max_vv, range_vv, mean_vv])\n",
        "    \n",
        "#         min_vh = min(row[1])\n",
        "#         max_vh = max(row[1])\n",
        "#         range_vh = max_vh - min_vh\n",
        "#         mean_vh = np.mean(row[1])\n",
        "#         correlation_vh = sm.tsa.acf(row[1])[1]\n",
        "#         permutation_entropy_vh = permutation_entropy(row[1], dx=6, base=2, normalized=True)\n",
        "    \n",
        "#         min_vv_by_vh = min(row[2])\n",
        "#         max_vv_by_vh = max(row[2])\n",
        "#         range_vv_by_vh = max_vv_by_vh - min_vv_by_vh\n",
        "#         mean_vv_by_vh = np.mean(row[2])\n",
        "#         correlation_vv_by_vh = sm.tsa.acf(row[2])[1]\n",
        "#         permutation_entropy_vv_by_vh = permutation_entropy(row[2], dx=6, base=2, normalized=True)\n",
        "    return features_list"
      ],
      "id": "f6989e88-16d2-4220-aaa6-1cc1f63dd188"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "161bdcd0-7de5-4077-85f0-ef86476cfc2a"
      },
      "outputs": [],
      "source": [
        "def combine_two_datasets(dataset1,dataset2):\n",
        "    '''\n",
        "    Returns a  vertically concatenated dataset.\n",
        "    Attributes:\n",
        "    dataset1 - Dataset 1 to be combined \n",
        "    dataset2 - Dataset 2 to be combined\n",
        "    '''\n",
        "    data = pd.concat([dataset1,dataset2], axis=1)\n",
        "    return data"
      ],
      "id": "161bdcd0-7de5-4077-85f0-ef86476cfc2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49e2ce9f-b4ee-4497-ba25-04ecfd4bed1b"
      },
      "outputs": [],
      "source": [
        "def get_sentinel_data(longitude, latitude, season,assests):\n",
        "    \n",
        "    '''\n",
        "    Returns a list of VV,VH, VV/VH values for a given latitude and longitude over a given time period (based on the season)\n",
        "    Attributes:\n",
        "    longitude - Longitude\n",
        "    latitude - Latitude\n",
        "    season - The season for which band values need to be extracted.\n",
        "    assets - A list of bands to be extracted\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    bands_of_interest = assests\n",
        "    if season == 'SA':\n",
        "        time_slice = \"2022-07-01/2022-07-31\"\n",
        "    if season == 'WS':\n",
        "        time_slice = \"2022-02-01/2022-02-28\"\n",
        "        \n",
        "    vv_list = []\n",
        "    vh_list = []\n",
        "    red_list = []\n",
        "    nir_list = []\n",
        "    blue_list = []\n",
        "    vv_by_vh_list = []\n",
        "    \n",
        "    bbox_of_interest = [longitude , latitude, longitude, latitude]\n",
        "    time_of_interest = time_slice\n",
        "    \n",
        "    catalog = pystac_client.Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\")\n",
        "    search = catalog.search(collections=[\"sentinel-1-rtc\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
        "    # search2 = catalog.search(collections=[\"alos-palsar-mosaic\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
        "    # search3 = catalog.search(collections=[\"modis-17A2HGF-061\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
        "    search4 = catalog.search(collections=[\"landsat-c2-l2\"], bbox=bbox_of_interest, datetime=time_of_interest)\n",
        "    items = list(search.get_all_items())\n",
        "    items4 = list(search4.get_all_items())\n",
        "    item = items[0]\n",
        "    items.reverse()\n",
        "    item4 = items4[0]\n",
        "    items.reverse()\n",
        "    \n",
        "    data = stac_load([items[1]],bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    selected_item = min(items4, key=lambda item: eo.ext(item).cloud_cover)\n",
        "    # bands_of_interest = assests\n",
        "    # data = stac_load([items[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    # data2 = stac_load([items2[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    # data3 = stac_load([items3[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    data4 = stac_load([selected_item], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    \n",
        "    for item in items:\n",
        "        data = stac_load([item], bands=bands_of_interest, patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "        if(data['vh'].values[0][0]!=-32768.0 and data['vv'].values[0][0]!=-32768.0):\n",
        "            data = data.where(~data.isnull(), 0)\n",
        "            vh = data[\"vh\"].astype(\"float64\")\n",
        "            vv = data[\"vv\"].astype(\"float64\")\n",
        "            vv_list.append(np.median(vv))\n",
        "            vh_list.append(np.median(vh))\n",
        "            vv_by_vh_list.append(np.median(vv)/np.median(vh))\n",
        "            \n",
        "    for item in items4:\n",
        "        data = stac_load([item], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "        if(data['red'].values[0][0]!=-32768.0 and data['blue'].values[0][0]!=-32768.0):\n",
        "            data = data.where(~data.isnull(), 0)\n",
        "            red = data[\"red\"].astype(\"float64\")\n",
        "            blue = data[\"blue\"].astype(\"float64\")\n",
        "            nir = data[\"nir08\"].astype(\"float64\")\n",
        "            red_list.append(np.median(red))\n",
        "            blue_list.append(np.median(blue))\n",
        "            nir_list.append(np.median(nir))\n",
        "              \n",
        "    return vv_list, vh_list, vv_by_vh_list, red_list, blue_list, nir_list"
      ],
      "id": "49e2ce9f-b4ee-4497-ba25-04ecfd4bed1b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27f49e87-df63-409c-a4f9-9afaf1cdb945"
      },
      "outputs": [],
      "source": [
        "def get_sentinel_data(lat, long, season, assets):\n",
        "    '''\n",
        "    Returns VV and VH values for a given latitude and longitude \n",
        "    Attributes:\n",
        "    latlong - A tuple with 2 elements - latitude and longitude\n",
        "    time_slice - Timeframe for which the VV and VH values have to be extracted\n",
        "    assets - A list of bands to be extracted\n",
        "    '''\n",
        "    # latlong=latlong.replace('(','').replace(')','').replace(' ','').split(',')\n",
        "    bbox_of_interest = [long , lat, long, lat]\n",
        "    if season == 'SA':\n",
        "        time_slice = \"2022-07-01/2022-07-31\"\n",
        "    if season == 'WS':\n",
        "        time_slice = \"2022-02-01/2022-02-28\"\n",
        "    time_of_interest = time_slice\n",
        "    time_of_interest2 = \"2020-01-01/2020-12-31\"\n",
        "    catalog = pystac_client.Client.open(\n",
        "        \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
        "    )\n",
        "    search = catalog.search(\n",
        "        collections=[\"sentinel-1-rtc\"], bbox=bbox_of_interest, datetime=time_of_interest\n",
        "    )\n",
        "    search2 = catalog.search(\n",
        "        collections=[\"alos-palsar-mosaic\"], bbox=bbox_of_interest, datetime=time_of_interest\n",
        "    )\n",
        "    # Extract year and month from format 2023-01-01\n",
        "    search3 = catalog.search(\n",
        "        collections=[\"modis-17A2HGF-061\"], bbox=bbox_of_interest, datetime=time_of_interest\n",
        "    )\n",
        "    search4 = catalog.search(\n",
        "        collections=[\"landsat-c2-l2\"], bbox=bbox_of_interest, datetime=time_of_interest2,\n",
        "        query={\"eo:cloud_cover\": {\"lt\": 10}},\n",
        "    )\n",
        "    items = list(search.get_all_items())\n",
        "    items2 = list(search2.get_all_items())\n",
        "    items3 = list(search3.get_all_items())\n",
        "    items4 = list(search4.item_collection())\n",
        "    selected_item = min(items4, key=lambda item: eo.ext(item).cloud_cover)\n",
        "    print('items', items)\n",
        "    print('items2', items2)\n",
        "    print('items3', items3)\n",
        "    print('items4', items4)\n",
        "    # bands_of_interest = assests\n",
        "    data = stac_load([items[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    # data2 = stac_load([items2[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    # data3 = stac_load([items3[0]], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    data4 = stac_load([selected_item], patch_url=pc.sign, bbox=bbox_of_interest).isel(time=0)\n",
        "    vh = data[\"vh\"].astype(\"float\").values.tolist()[0][0]\n",
        "    vv = data[\"vv\"].astype(\"float\").values.tolist()[0][0]\n",
        "    # hh = data2[\"HH\"].astype(\"float\").values.tolist()[0][0]\n",
        "    # hv = data2[\"HV\"].astype(\"float\").values.tolist()[0][0]\n",
        "    # gpp = data3[\"Gpp_500m\"].astype(\"float\").values.tolist()[0][0]\n",
        "    # psn = data3[\"PsnNet_500m\"].astype(\"float\").values.tolist()[0][0]\n",
        "    nir = data4[\"nir08\"].astype(\"float\").values.tolist()[0][0]\n",
        "    red = data4[\"red\"].astype(\"float\").values.tolist()[0][0]\n",
        "    blue = data4[\"blue\"].astype(\"float\").values.tolist()[0][0]\n",
        "    return vh,vv, nir, red, blue\n",
        "\n",
        "\n"
      ],
      "id": "27f49e87-df63-409c-a4f9-9afaf1cdb945"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87152e20-7a22-4a09-a656-9a1cb2b3d66a"
      },
      "outputs": [],
      "source": [
        "def generate_feature_name(start_df):\n",
        "    columns = []\n",
        "    for i in start_df.columns: \n",
        "        columns = columns + [f'min_{i}', f'max_{i}', f'range_{i}', f'mean_{i}']\n",
        "    return columns\n",
        "        "
      ],
      "id": "87152e20-7a22-4a09-a656-9a1cb2b3d66a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ad9c4b-4c9e-45e9-a6bf-5a53b84485f2"
      },
      "source": [
        "### Retrieve the input csv"
      ],
      "id": "e6ad9c4b-4c9e-45e9-a6bf-5a53b84485f2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "318f359b-9cf5-4b90-b1fb-16cd4516dbb9"
      },
      "source": [
        "## We do not allow to use season, latlong, district name as out predictor"
      ],
      "id": "318f359b-9cf5-4b90-b1fb-16cd4516dbb9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22ef659a-0074-47d7-88b7-385d863680a2"
      },
      "outputs": [],
      "source": [
        "crop_yield_data = pd.read_csv(\"/content/drive/MyDrive/Untitled Folder 1/Crop_Yield_Data_challenge_2.csv\")\n",
        "crop_yield_data.head()"
      ],
      "id": "22ef659a-0074-47d7-88b7-385d863680a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "987de33b-986e-4ffd-b99b-ded1cbb986e8"
      },
      "outputs": [],
      "source": [
        "crop_yield_data.groupby('District').head()"
      ],
      "id": "987de33b-986e-4ffd-b99b-ded1cbb986e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbf4786c-b8a4-4b26-a563-4b515df21ad9"
      },
      "outputs": [],
      "source": [
        "def retrieve_data(input_df) :\n",
        "    assests = ['vh','vv']\n",
        "    train_band_values=crop_yield_data.progress_apply(lambda x: get_sentinel_data(x['Latitude'],x['Longitude'], x['Season(SA = Summer Autumn, WS = Winter Spring)'],assests), axis=1)\n",
        "    vh = [x[0] for x in train_band_values]\n",
        "    vv = [x[1] for x in train_band_values]\n",
        "    # vv_by_vh = [x[2] for x in train_band_values]\n",
        "    red = [x[3] for x in train_band_values]\n",
        "    blue = [x[4] for x in train_band_values]\n",
        "    nir = [x[5] for x in train_band_values]\n",
        "    vh_vv_data = pd.DataFrame(list(zip(vh,vv, red, blue, nir)),columns = [\"vv_list\",\"vh_list\",\"red\",\"blue\",\"nir\"])\n",
        "    return vh_vv_data"
      ],
      "id": "cbf4786c-b8a4-4b26-a563-4b515df21ad9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98942b86-3b60-48d0-be0f-e428445bd2b6"
      },
      "outputs": [],
      "source": [
        "# vh_vv_data = retrieve_data(crop_yield_data)"
      ],
      "id": "98942b86-3b60-48d0-be0f-e428445bd2b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d84aa288-5edc-4f60-8041-a4a16e6fa34a"
      },
      "source": [
        "### Do some statistical plotting"
      ],
      "id": "d84aa288-5edc-4f60-8041-a4a16e6fa34a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20474114-a710-438c-b802-e20cb7cebf94"
      },
      "outputs": [],
      "source": [
        "# vh_vv_data.to_parquet('/content/drive/MyDrive/Untitled Folder 1/vh_vv.gzip', compression='gzip', index = False)"
      ],
      "id": "20474114-a710-438c-b802-e20cb7cebf94"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Here if finished downloading"
      ],
      "metadata": {
        "id": "WJQdAAxcxsTU"
      },
      "id": "WJQdAAxcxsTU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7447c1cf-d23b-45bc-b90b-7fd44b839bba"
      },
      "outputs": [],
      "source": [
        "vh_vv_data = pd.read_parquet('/content/drive/MyDrive/Untitled Folder 1/vh_vv.gzip')"
      ],
      "id": "7447c1cf-d23b-45bc-b90b-7fd44b839bba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9423bda1-552c-46ec-9a1a-60cd8a993827"
      },
      "outputs": [],
      "source": [
        "vh_vv_data.info()"
      ],
      "id": "9423bda1-552c-46ec-9a1a-60cd8a993827"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ebe198c-b9a2-4753-b0f5-82ea2a41028b"
      },
      "source": [
        "## Feature Engineering"
      ],
      "id": "0ebe198c-b9a2-4753-b0f5-82ea2a41028b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d22d40f1-0c42-44c8-8a72-0ac7ed6036b2"
      },
      "outputs": [],
      "source": [
        "def reshape_list_columns(df):\n",
        "    \"\"\"\n",
        "    Reshape any columns containing lists into multiple columns.\n",
        "    Each element in the original lists will become a new column\n",
        "    with a name based on the original column name and the element index.\n",
        "    \n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): the DataFrame to reshape\n",
        "        \n",
        "    Returns:\n",
        "        pandas.DataFrame: the reshaped DataFrame\n",
        "    \"\"\"\n",
        "    # Loop through each column in the DataFrame\n",
        "    for col in df.columns:\n",
        "        print(col)\n",
        "        # Check if the column contains lists\n",
        "        if df[col].dtype == 'object':\n",
        "            print('Reach here')\n",
        "            # Determine the maximum number of elements in the lists\n",
        "            max_len = df[col].apply(len).max()\n",
        "            # Create new column names for each element in the lists\n",
        "            new_cols = [f\"{col}_{i+1}\" for i in range(max_len)]\n",
        "            # Create a new DataFrame with the reshaped lists\n",
        "            new_df = pd.DataFrame(df[col].to_list(), columns=new_cols)\n",
        "            # Concatenate the new DataFrame with the original DataFrame\n",
        "            df = pd.concat([df, new_df], axis=1)\n",
        "            # Drop the original column from the DataFrame\n",
        "            df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "id": "d22d40f1-0c42-44c8-8a72-0ac7ed6036b2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afbf3833-4858-4aa8-b91d-e39472c97264"
      },
      "outputs": [],
      "source": [
        "def ndvi(row):\n",
        "    nir = row['nir_1']\n",
        "    red = row['red_1']\n",
        "    return (nir - red) / (nir + red)\n",
        "\n",
        "# Apply the function to each row in the DataFrame"
      ],
      "id": "afbf3833-4858-4aa8-b91d-e39472c97264"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fd365c5-54dc-40f1-950a-ee989c6f074e"
      },
      "outputs": [],
      "source": [
        "def evi(row):\n",
        "    nir = row['nir_1']\n",
        "    red = row['red_1']\n",
        "    blue = row['blue_1']\n",
        "    return 2.5 * ((nir - red) / (nir + 6 * red - 7.5 * blue + 1))\n",
        "\n",
        "# Apply the function to each row in the DataFrame\n"
      ],
      "id": "7fd365c5-54dc-40f1-950a-ee989c6f074e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9ac5e3d-1cfe-42fb-8731-4b649f096204"
      },
      "outputs": [],
      "source": [
        "def generate_trainning_dataset(input_df) :\n",
        "    avoid_list = ['Season(SA = Summer Autumn, WS = Winter Spring)','Rice Crop Intensity(D=Double, T=Triple)','Latitude','Longitude', 'Date of Harvest']\n",
        "    name_cols = list()\n",
        "    for i in input_df.columns : \n",
        "        if i in avoid_list:\n",
        "            continue\n",
        "        name_cols.append(i)\n",
        "    return input_df[name_cols]"
      ],
      "id": "f9ac5e3d-1cfe-42fb-8731-4b649f096204"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e60aa24-c014-42d2-a954-7c4605890d62"
      },
      "outputs": [],
      "source": [
        "def feature_engineering(parquet, crop_yield_data):\n",
        "    res = reshape_list_columns(parquet).dropna(axis=1)\n",
        "    res['ndvi'] = res.apply(ndvi, axis=1)\n",
        "    res['evi'] = res.apply(evi, axis=1)\n",
        "    crop_yield_data['Date of Harvest'] = pd.to_datetime(crop_yield_data['Date of Harvest'], format='%d-%m-%Y')\n",
        "    res = combine_two_datasets(res, crop_yield_data)\n",
        "    dummy_df = pd.get_dummies(res['Rice Crop Intensity(D=Double, T=Triple)'], prefix='Rice Crop Intensity')\n",
        "    res = combine_two_datasets(res, dummy_df)\n",
        "    res = generate_trainning_dataset(res)\n",
        "    return res\n",
        "    "
      ],
      "id": "7e60aa24-c014-42d2-a954-7c4605890d62"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fda6b8b-4afb-45c2-9aa4-ee5c31ce58f3"
      },
      "outputs": [],
      "source": [
        "trainning_df = feature_engineering(vh_vv_data, crop_yield_data)"
      ],
      "id": "6fda6b8b-4afb-45c2-9aa4-ee5c31ce58f3"
    },
    {
      "cell_type": "code",
      "source": [
        "district_groups = trainning_df.groupby('District')"
      ],
      "metadata": {
        "id": "i98lgGuMiQHF"
      },
      "id": "i98lgGuMiQHF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "district_dfs = [group_df for district, group_df in district_groups]"
      ],
      "metadata": {
        "id": "ykW0PKEriQCS"
      },
      "id": "ykW0PKEriQCS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(district_dfs)"
      ],
      "metadata": {
        "id": "WZB4V9kpiP1e"
      },
      "id": "WZB4V9kpiP1e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## heatmeap to see the correlation between features. \n",
        "# Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
        "mask = np.zeros_like(district_dfs[0].corr(), dtype=np.bool)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "sns.set_style('whitegrid')\n",
        "plt.subplots(figsize = (15,12))\n",
        "sns.heatmap(district_dfs[0].corr(), annot=True, mask = mask,\n",
        "            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n",
        "            linewidths=.9, linecolor='white', fmt='.2g', center = 0, square=True)\n",
        "plt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40)"
      ],
      "metadata": {
        "id": "s63TWjTWOdFc"
      },
      "id": "s63TWjTWOdFc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eee8feb-f738-497f-ac51-88d842e7723e"
      },
      "source": [
        "## Handling outlier"
      ],
      "id": "0eee8feb-f738-497f-ac51-88d842e7723e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44e37b17-51ea-400e-8d62-d0b1e76b6b08"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in district_dfs :\n",
        "  sns.set(style=\"ticks\")\n",
        "  sns.pairplot(i)\n",
        "  plt.show()  "
      ],
      "id": "44e37b17-51ea-400e-8d62-d0b1e76b6b08"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56808cb0-d8eb-47a4-9148-9c0703d7aa4d"
      },
      "outputs": [],
      "source": [
        "def replace_outliers_with_mean(df):\n",
        "    avoid_dtypes = ['object','Timestamp']\n",
        "    for col in df.columns:\n",
        "        if col in ['red_1','nir_1','blue_1'] :\n",
        "            df[col] = np.where(df[col] == 0, df[col].mean(), df[col])\n",
        "        if df[col].dtype not in avoid_dtypes:\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            upper_bound = q3 + 1.5 * iqr\n",
        "            lower_bound = q1 - 1.5 * iqr\n",
        "            df[col] = np.where(df[col] > upper_bound, df[col].mean(), df[col])\n",
        "            df[col] = np.where(df[col] < lower_bound, df[col].mean(), df[col])\n",
        "    return df"
      ],
      "id": "56808cb0-d8eb-47a4-9148-9c0703d7aa4d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3794ceaa-36ea-45f5-8ac0-f3d0d2ee36c9"
      },
      "outputs": [],
      "source": [
        "def replace_outliers_zscore(df, threshold=3):\n",
        "    \"\"\"\n",
        "    Replace outliers in a pandas DataFrame using the Z-score method with mean value of the column.\n",
        "    \n",
        "    Parameters:\n",
        "    df (pandas DataFrame): The DataFrame to clean.\n",
        "    threshold (float): The Z-score threshold for outlier detection. Data points with a Z-score greater than the\n",
        "        threshold will be replaced. Default is 3.\n",
        "    \n",
        "    Returns:\n",
        "    pandas DataFrame: The cleaned DataFrame with outliers replaced.\n",
        "    \"\"\"\n",
        "    df_cleaned = df.copy()\n",
        "    for col in df_cleaned.columns:\n",
        "        zscore = np.abs((df_cleaned[col] - df_cleaned[col].mean()) / df_cleaned[col].std())\n",
        "        mean_value = df_cleaned[col].mean()\n",
        "        df_cleaned.loc[zscore > threshold, col] = mean_value\n",
        "    return df_cleaned"
      ],
      "id": "3794ceaa-36ea-45f5-8ac0-f3d0d2ee36c9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "472d5cca-3d1d-4b76-ab75-a7b9c9b69a72"
      },
      "outputs": [],
      "source": [
        "def normalize_df_arr(input_arr, outlier_replacement) :\n",
        "  res = []\n",
        "  for i in input_arr :\n",
        "    tmp = i.drop('District', axis = 1)\n",
        "    res.append(outlier_replacement(tmp))\n",
        "  return res"
      ],
      "id": "472d5cca-3d1d-4b76-ab75-a7b9c9b69a72"
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_dfs = normalize_df_arr(district_dfs, replace_outliers_zscore)"
      ],
      "metadata": {
        "id": "dEu_hFFlgyp_"
      },
      "id": "dEu_hFFlgyp_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wd0WIVizklST"
      },
      "id": "wd0WIVizklST",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6de7e18-2d12-4523-ad3b-8c69dc75ce93"
      },
      "outputs": [],
      "source": [
        "# normalize_df['ndvi'] = normalize_df.apply(ndvi, axis=1)\n",
        "# normalize_df['evi'] = normalize_df.apply(evi, axis=1)"
      ],
      "id": "e6de7e18-2d12-4523-ad3b-8c69dc75ce93"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d152e5da-fe7c-4b0d-9248-5b0d45393f70"
      },
      "outputs": [],
      "source": [
        "# sns.set(style=\"ticks\")\n",
        "# sns.pairplot(normalize_df_2)\n",
        "# plt.show()"
      ],
      "id": "d152e5da-fe7c-4b0d-9248-5b0d45393f70"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da9a1140-c220-42df-ba7e-395eae56ef24"
      },
      "outputs": [],
      "source": [
        "X = normalize_dfs[0].drop('Rice Yield (kg/ha)', axis=1)  # Features\n",
        "y = normalize_dfs[0]['Rice Yield (kg/ha)']  # Target variable\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)"
      ],
      "id": "da9a1140-c220-42df-ba7e-395eae56ef24"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape[1]"
      ],
      "metadata": {
        "id": "BdSkEPWvqksk"
      },
      "id": "BdSkEPWvqksk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "m6TOajIJ553O"
      },
      "id": "m6TOajIJ553O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def objective(trial):\n",
        "    \n",
        "    # scalers = trial.suggest_categorical(\"scalers\", ['standard','minmax','robust',None])\n",
        "    # perform feature selection\n",
        "    feature_select = trial.suggest_categorical(\"feature_selector\", [\"selectkbest\", \"none\"])\n",
        "    \n",
        "    featureK= trial.suggest_int(\"k\", 1, X_train.shape[1])\n",
        "    print(f'featurek{featureK}')\n",
        "    if feature_select == \"selectkbest\":\n",
        "        feature_selector = SelectKBest(score_func=f_classif ,k=featureK)\n",
        "    else :\n",
        "        feature_selector = 'passthrough'\n",
        "\n",
        "    # (b) Define your scalers\\\n",
        "    scaler = MinMaxScaler()\n",
        "    # if scalers == \"minmax\":\n",
        "    #     scaler = MinMaxScaler()\n",
        "    # elif scalers == \"standard\":\n",
        "    #     scaler = StandardScaler()\n",
        "    # elif scalers == \"robust\" :\n",
        "    #     scaler = RobustScaler()\n",
        "    # else:\n",
        "    #     scaler = 'passthrough'\n",
        "    \n",
        "    # -- Instantiate dimensionality reduction\n",
        "     # (a) List all dimensionality reduction options\n",
        "    dim_red = trial.suggest_categorical(\"dim_red\", [\"PCA\",\"TruncatedSVD\", None])\n",
        "    n_components=trial.suggest_int(\"n_components\", 1, featureK)\n",
        "    # (b) Define the PCA algorithm and its hyperparameters\n",
        "    if featureK == 1 or featureK >= n_components:\n",
        "        dimen_red_algorithm='passthrough'\n",
        "    else:\n",
        "        if dim_red == \"PCA\":\n",
        "            dimen_red_algorithm=PCA(n_components=n_components)\n",
        "        elif dim_red == \"TruncatedSVD\":\n",
        "            dimen_red_algorithm=TruncatedSVD(n_components=n_components)\n",
        "        # (c) No dimensionality reduction option\n",
        "        else:\n",
        "            dimen_red_algorithm='passthrough'\n",
        "        \n",
        "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        (\"impute\",imp),\n",
        "        (\"scaler\", scaler),\n",
        "        (\"feature_selector\",feature_selector),\n",
        "        (\"dim_red\", dimen_red_algorithm),\n",
        "        #Can change if gpu support is implementted\n",
        "        (\"catboost\", CatBoostRegressor())\n",
        "        \n",
        "    ])\n",
        "    \n",
        "    # Parameter for tunning lightgbm\n",
        "    params = {\n",
        "        'catboost__iterations': 1000,\n",
        "        'catboost__learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
        "        'catboost__depth': trial.suggest_int('depth', 4, 10),\n",
        "        'catboost__l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 10),\n",
        "        'catboost__bagging_temperature': trial.suggest_loguniform('bagging_temperature', 1e-3, 10),\n",
        "        'catboost__border_count': trial.suggest_int('border_count', 32, 255),\n",
        "        'catboost__verbose': False,\n",
        "        'catboost__random_seed': 42\n",
        "    }\n",
        "    \n",
        "    # params = {\n",
        "    #     \"catboost__iterations\": trial.suggest_int(\"iterations\", 100, 1000),\n",
        "    #     \"catboost__learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.1),\n",
        "    #     \"catboost__depth\": trial.suggest_int(\"depth\", 4, 10),\n",
        "    #     \"catboost__l2_leaf_reg\": trial.suggest_loguniform(\"l2_leaf_reg\", 1e-3, 10),\n",
        "    #     \"catboost__bagging_temperature\": trial.suggest_loguniform(\"bagging_temperature\", 0.1, 100),\n",
        "    #     \"catboost__random_strength\": trial.suggest_loguniform(\"random_strength\", 1e-3, 10),\n",
        "    #     \"catboost__border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
        "    #     \"catboost__thread_count\": -1,\n",
        "    #     \"catboost__loss_function\": \"RMSE\",\n",
        "    #     \"catboost__eval_metric\": \"RMSE\",\n",
        "    #     \"catboost__verbose\": False,\n",
        "    # }\n",
        "\n",
        "    # Fit the model\n",
        "    model = pipeline.set_params(**params)\n",
        "    # model.fit(X_train, y_train)\n",
        "    # y_pred = model.predict(X_val)\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    scores = r2_score(y_pred, y_test)\n",
        " \n",
        "    try:\n",
        "        scores = r2_score(y_pred, y_test)\n",
        "        return scores\n",
        "    except:\n",
        "        return 0\n",
        "    \n",
        "study = optuna.create_study(direction='maximize',\n",
        "                            storage=\"sqlite:///db.sqlite3\",  # Specify the storage URL here.\n",
        "                            study_name=datetime.utcnow())"
      ],
      "metadata": {
        "id": "3Fr8_RvDOpnc"
      },
      "id": "3Fr8_RvDOpnc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study.optimize(objective, n_trials=500)"
      ],
      "metadata": {
        "id": "yQUo13SaOwsX"
      },
      "id": "yQUo13SaOwsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "   def print_res(input_study):\n",
        "    print('Number of finished trials: {}'.format(len(input_study.trials)))\n",
        "    print('Best trial:')\n",
        "    trial = input_study.best_trial\n",
        "\n",
        "    print('  Value: {}'.format(trial.value))\n",
        "    print('  Params: ')\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        print('    {}: {}'.format(key, value))\n",
        "\n",
        "print_res(study)"
      ],
      "metadata": {
        "id": "39etUKqqOwh1"
      },
      "id": "39etUKqqOwh1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the result\n",
        "def get_params(input_study) :\n",
        "    params = {k: v for k, v in input_study.best_params.items() if k not in ('dim_red', 'scalers')}\n",
        "    change = []\n",
        "    for k,v in dict(params).items():\n",
        "        tmp_name = k\n",
        "        if 'catboost' not in tmp_name :\n",
        "            res = f\"catboost__{tmp_name}\"\n",
        "            params[res] = params.pop(tmp_name)\n",
        "            change.append(res)\n",
        "    params.pop('catboost__n_components')\n",
        "    params.pop('catboost__k')\n",
        "    params.pop('catboost__feature_selector')\n",
        "    return params\n",
        "\n",
        "params = get_params(study)"
      ],
      "metadata": {
        "id": "vIsHWUqkteKh"
      },
      "id": "vIsHWUqkteKh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "Fb0DRafTt-nq"
      },
      "id": "Fb0DRafTt-nq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "        (\"impute\",SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
        "        (\"scaler\", MinMaxScaler()),\n",
        "        (\"feature_selector\",SelectKBest(score_func=f_classif ,k=5)),\n",
        "        #Can change if gpu support is implementted\n",
        "        (\"catboost\", CatBoostRegressor())\n",
        "    ])"
      ],
      "metadata": {
        "id": "NPf8nljXuVAN"
      },
      "id": "NPf8nljXuVAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.set_params(**params)"
      ],
      "metadata": {
        "id": "0JjadClZuh9k"
      },
      "id": "0JjadClZuh9k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "fbeFwqG0vuN_"
      },
      "id": "fbeFwqG0vuN_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a744ce91-4ee6-4d36-aa10-98b4937aad73"
      },
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)"
      ],
      "id": "a744ce91-4ee6-4d36-aa10-98b4937aad73"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0fa17f5-a12a-474f-b8d4-91e2146f47ca"
      },
      "outputs": [],
      "source": [
        "# from sklearn.metrics import mean_absolute_percentage_error\n",
        "# from sklearn.metrics import r2_score\n",
        "# mean_absolute_percentage_error(y_pred, y_test)\n"
      ],
      "id": "a0fa17f5-a12a-474f-b8d4-91e2146f47ca"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ce65ebb-962e-4473-baa3-1b1bc31c96cf"
      },
      "outputs": [],
      "source": [
        "r2_score(y_pred, y_test)"
      ],
      "id": "7ce65ebb-962e-4473-baa3-1b1bc31c96cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bc9890c-21eb-414f-bf3e-ed8ed931d336"
      },
      "source": [
        "### Generate the verification dataset"
      ],
      "id": "3bc9890c-21eb-414f-bf3e-ed8ed931d336"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd29b120-c443-4d9d-83c6-05d19c107f6c"
      },
      "outputs": [],
      "source": [
        "result = pd.read_csv('/content/drive/MyDrive/Untitled Folder 1/Challenge_2_submission_template.csv')"
      ],
      "id": "dd29b120-c443-4d9d-83c6-05d19c107f6c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b8f1296-892f-4e16-8836-c65e81ee2949"
      },
      "outputs": [],
      "source": [
        "result.head()"
      ],
      "id": "2b8f1296-892f-4e16-8836-c65e81ee2949"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d7c7903-a620-4db0-aca6-4f8b1fd47c31"
      },
      "outputs": [],
      "source": [
        "result.drop('Predicted Rice Yield (kg/ha)', axis = 1, inplace = True)"
      ],
      "id": "6d7c7903-a620-4db0-aca6-4f8b1fd47c31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05de322b-0627-4b81-8a25-db8aa191feed"
      },
      "outputs": [],
      "source": [
        "# vh_vv_result_data = retrieve_data(result)"
      ],
      "id": "05de322b-0627-4b81-8a25-db8aa191feed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eae533c9-1be0-4f0d-9a4d-a79468c3e60c"
      },
      "outputs": [],
      "source": [
        "# vh_vv_result_data.to_parquet('/content/drive/MyDrive/Untitled Folder 1/vh_vv_result_data.gzip', compression='gzip', index = False)"
      ],
      "id": "eae533c9-1be0-4f0d-9a4d-a79468c3e60c"
    },
    {
      "cell_type": "code",
      "source": [
        "vh_vv_result_data = pd.read_parquet('/content/drive/MyDrive/Untitled Folder 1/vh_vv_result_data.gzip')"
      ],
      "metadata": {
        "id": "ZSGN_OhHu-cA"
      },
      "id": "ZSGN_OhHu-cA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17b74910-35ad-494a-9de0-115d2a82c6e8"
      },
      "outputs": [],
      "source": [
        "trainning_df = feature_engineering(vh_vv_result_data, result)"
      ],
      "id": "17b74910-35ad-494a-9de0-115d2a82c6e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3865b4f-2ada-448c-9db0-a4bad19b1332"
      },
      "outputs": [],
      "source": [
        "final = trainning_df.drop(['ID No','District'], axis = 1)"
      ],
      "id": "d3865b4f-2ada-448c-9db0-a4bad19b1332"
    },
    {
      "cell_type": "code",
      "source": [
        "final.head()"
      ],
      "metadata": {
        "id": "I2TUEvpgBUa3"
      },
      "id": "I2TUEvpgBUa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6e88417-36a1-40f2-a11b-a63387c5cad3"
      },
      "outputs": [],
      "source": [
        "final_prediction_series = pd.Series(pipeline.predict(final))"
      ],
      "id": "f6e88417-36a1-40f2-a11b-a63387c5cad3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450f5e4e-667d-48f0-8c58-cdec80b454a3"
      },
      "outputs": [],
      "source": [
        "result['Predicted Rice Yield (kg/ha)'] = final_prediction_series"
      ],
      "id": "450f5e4e-667d-48f0-8c58-cdec80b454a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21f34a79-b346-4b15-8e10-5cea712507e2"
      },
      "outputs": [],
      "source": [
        "result.head()"
      ],
      "id": "21f34a79-b346-4b15-8e10-5cea712507e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa6c7e4c-d63d-4375-b9f9-9193a5f670a6"
      },
      "outputs": [],
      "source": [
        "result.to_csv('/content/drive/MyDrive/Untitled Folder 1/submission_not_tune.csv', index= False)"
      ],
      "id": "fa6c7e4c-d63d-4375-b9f9-9193a5f670a6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a4accdf-e310-41b2-818b-1c4f3343c453"
      },
      "outputs": [],
      "source": [],
      "id": "0a4accdf-e310-41b2-818b-1c4f3343c453"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}